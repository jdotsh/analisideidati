\documentclass[11pt]{article}
\usepackage[italian]{babel}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[normalem]{ulem}
\newcommand{\numpy}{{\tt numpy}}    % tt font for numpy

\topmargin -.5in
\textheight 9in
\oddsidemargin -.25in
\evensidemargin -.25in
\textwidth 7in

\begin{document}

% ========== Edit your name here
\author{What are the odds?}
\title{Manuale dell'ingegnere intrippato con la statistica}

\maketitle

\medskip
\section{Statistica descrittiva}
\subsection{Le grandezze che sintetizzano i dati}
\subsubsection{Media}
Dato un insieme $x_1,x_2,...,x_n$ di dati, si dice media campionaria la media aritmetica di questi valori.
\begin{displaymath}
\overline{x}:=\frac{1}{n}\sum_{i=1}^{n}x_i
\end{displaymath}
\subsubsection{Mediana}
Dato un insieme di dati di ampiezza $n$, lo si ordini dal minore al maggiore. La mediana è il valore che occupa la posizioone $\frac{n+1}{2}$ in caso di un insieme dispari, o la media tra $\frac{n}{2}$ e $\frac{n}{2}+1$ se pari.
\subsubsection{Moda}
La moda campionaria di un insieme di dati, se esiste, è l'unico valore che ha frequenza massima.
\subsubsection{Varianza e deviazione standard campionarie}
Dato un insieme di dati $x_1,x_2,...,x_n$, si dice varianza campionaria ($s^2$), la quantità
\begin{displaymath}
s^2:=\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2
\end{displaymath}
Una comodità per il calcolo è che 
\begin{displaymath}
\sum_{i=1}^{n}(x_i-\overline{x})^2 = \sum_{i=1}^{n}x_i^2-n\overline{x}^2
\end{displaymath}
Si dice \textbf{deviazione standard campionaria} e si denota con $s$, la quantità
\begin{displaymath}
s:=\sqrt{\frac{1}{n-1}\sum_{i=1}^{n}(x_i-\overline{x})^2}
\end{displaymath}
\begin{center}
    (la radice quadrata di $s^2$)
\end{center}
\subsubsection{Percentili campionari e box plot}
Sia k un numero intero $0\le k \le 100$. Dato un campione di dati, esiste sempre un dato che è contemporaneamente maggiore del $k$ percento dei dati, e minore del $100-k$ percento. Per trovare questo dato, dati $n$ e $p=\frac{k}{100}$:
\begin{enumerate}
    \item Disponiamo i dati in ordine crescente
    \item Calcoliamo $np$
    \item Il numero cercato è quello in posizione $np$, arrotondato per eccesso se non intero.
\end{enumerate}
Il 25-esimo percentile si dice \textit{primo quartile}, il 50-esimo \textit{secondo} (ed è pari alla mediana), il 75-esimo \textit{terzo}. Il box plot è un grafica con un quadrato sulla linea dei dati, con i lati sul primo e terzo quartile, e un segno sul secondo.
\subsection{Disuguaglianza di Chebyshev}
Siano $\overline{x}$ e $s$ media e deviazione standard campionarie di un insieme di dati. Nell'ipotesi che $s>0$, la disuguaglianza di Chebyshev afferma che per ogni reale $k\ge 1$, almeno una frazione $(1-1/k^2)$ dei dati cade nell'intervallo che va da $\overline{x}-ks$ a $\overline{x}+ks$.
Usando il \sout{pessimo} \textit{fantastico} linguaggio da statista: sia assegnato un insieme di dati $x_1,...,x_n$ con media campionaria $\overline{x}$ e deviazione standard campionaria $s>0$. Denotiamo con $S_k$ l'insieme degli indici corrispondenti a dati compresi tra $\overline{x}-ks$ e $\overline{x}+ks$. Sia $\#S_k$ il numero dei suddetti. Allora abbiamo che
\begin{displaymath}
\frac{\#S_k}{n}\ge 1 - \frac{n-1}{nk^2} >1-\frac{1}{k^2}
\end{displaymath}
\subsection{Insiemi di dati bivariati e coefficiente di correlazione campionaria}
A volte non abbiamo a che fare con dati singoli, ma con coppie di numeri, tra i quali sospettiamo l'esistenza di relazioni. Dati di questa forma prendono il nome di \textit{campione bivariato}. Uno strumento utile è il diagramma di dispersione. Una questione interessante è capire se vi sia correlazione tra i dati accoppiati. Parleremo di correlazione positiva quando abbiamo una proporzionalità diretta tra i due, di correlazione negativa quando abbiamo una proporzionalità inversa.
\subsubsection{Coefficiente di correlazione campionaria}
Dato un campione bivariato $(x_i,y_i)$, sono definite le medie $\overline{x}$ e $\overline{y}$. Possiamo senz'altro dire che se un valore $x_i$ è grande rispetto alla media, la differenza $x_i-\overline{x}$ sarà positiva, mentre se $x_i$ è piccolo, la differenza sarà negativa. Quindi, considerando il prodotto $(x_i-\overline{x})(y_i-\overline{y})$, sarà positivo per correlazioni positive, negativo per correlazioni negative. Se l'intero campione mostra quindi un'elevata correlazione, ci aspettiamo che la somma di tutti i prodotti $\sum_{i=1}^n(x_i-\overline{x})(y_i-\overline{y})$ darà una buona stima della correlazione. Normalizziamola dividendo per $(n-1)$ e per il prodotto delle deviazione standard campionarie, e otteniamo il \textbf{coefficiente di correlazione campionaria}
\begin{displaymath}
r:=\frac{\sum_{i=1}^{n}(x_i-\overline{x})(y_i-\overline{y})}{(n-1)s_x s_y}
\end{displaymath}
con $s_x$ e $s_y$ deviazioni standard campionarie di $x$ e $y$.
\subsubsection{Proprietà del coefficiente di correlazione campionaria}
Sebbene parleremo meglio di questo bastardo nella sezione sulla regressione, elenchiamo qui alcune proprietà:
\begin{enumerate}
    \item $-1 \le r \le 1$
    \item Se per opportune costanti a e b, con $b>0$ sussiste la relazione lineare $y_i = a+b_x$, allora $r=1$.
    \item Se per opportune costanti a e b, con $b<0$ sussiste la relazione lineare $y_i = a+b_x$, allora $r=-1$.
    \item Se $r$ è il coefficiente di correlazione del campione $(x_i, y_i)$, $i=1,...,n$, allora lo è anche per il campione $(a+bx_i, c+dy_i)$, purché le costanti $a$ e $b$ abbiano lo stesso segno.
\end{enumerate}
\section{Elementi di probabilità}
\subsection{Spazio degli esiti ed eventi}
Si dice spazio degli esiti l'insieme di tutti gli esiti possibili di un esperimento. Se ad esempio l'esito dell'esperimento fosse il sesso di un neonato, lo spazio degli esiti sarebbe 
\begin{displaymath}
S=\{f,m\}
\end{displaymath}
I sottoinsiemi dello spazio degli esiti si dicono \textbf{eventi}, quindi un evento E è un insieme i cui elementi sono esiti possibili. Si dice $E^c$ l'opposto dell'evento, quindi $P(E^c) = 1 - P(E)$. Risulta ovvio che $1 = P(E^c) + P(E)$.
Se abbiamo due eventi qualsiasi, la loro unione $P(E\cup F) = P(E) + P(F) - P(E\cap F)$.
\subsection{Spazi di esiti equiprobabili}
Per tanti esperimenti è naturale assumere che ognuno degli esiti abbia la stessa probabilità di accadere. Abbiamo quindi che la probabilità che $E$ accada è pari a $P(E)=\frac{\#E}{N}$.
\subsubsection{Principio di enumerazione}
Consideriamo la realizzazione di due diversi esperimenti che possono avere rispettivamente $m$ ed $n$ esiti. Allora complessivamente avremo $mn$ risultati.
\subsection{Coefficiente binomiale}
Vogliamo ora determinare il numero di diversi gruppi di $r$ oggetti che si possono formare scegliendoli da un insieme di $n$. Ad esempio, quanti gruppi di 3 lettere possono formarsi dal gruppo \{A,B,C,D,E\}. In generale, poiché il numero di modi diversi di scegliere $r$ oggetti su $n$ tenendo conto dell'ordine è dato da $n(n-1)...(n-r+1)$, e poiché ogni gruppo di lettere viene contato $r!$ volte (uno per permutazione), il numero di gruppi di $r$ elementi su n totali è dato da
\begin{displaymath}
    \frac{n(n-1)...(n-1+r)}{r!} = \frac{n!}{r!(n-r!)} = {n\choose r}
\end{displaymath}
\subsection{Probabilità condizionata}
Vogliamo ora calcolare la probabilità che un evento accada, appurato che ne è accaduto un altro. Ad esempio, lanciamo due dadi. L'evento E cercato è che il risultato sia 8. L'evento F già accaduto è che il primo dato risulta in un 3. Si dice probabilità condizionata di E dato F
\begin{displaymath}
    P(E|F) = \frac{P(E\cap F)}{P(F)}
\end{displaymath}
\subsection{Fattorizzazione di un evento e formula di Bayes}
Siano E ed F due eventi qualsiasi. È possibile esprimere E come
\begin{displaymath}
    P(E) = P(E\cap F) + P(E\cap F^c)
\end{displaymath}
Visto inoltre che i due sono eventi disgiunti, si ha che
\begin{gather*}
    P(E) = P(E|F)P(F) + P(E|F^c)P(F^c)\\
    P(E|F)P(F) + P(E|F^c)[1-P(F)]
\end{gather*}
Questa \textit{orribile} equazione, ci mostra che la probabilità dell'evento E si può ricavare come media pesata delle probabilità condizionali di E sapendo che: F si è verificato e non si è verificato. I pesi sono ovviamente le probabilità degli eventi a cui si condiziona.
\subsection{Eventi indipendenti}
Due eventi si dicono indipendenti quando il risultato di uno non influenza l'altro. In altre parole, significa che avendo due eventi E ed F, se so che F è accaduto, la probabilità che accada E non cambia.
\begin{displaymath}
    P(E\cap F) = P(E) P(F)
\end{displaymath}
\section{Variabili aleatorie e valore atteso}
Quando realizziamo un esperimento casuale, non sempre siamo interessati a tutti i risultati del suddetto. Se ad esempio lanciassimo due dadi, potrebbe interessarci la sola somma e non i singoli risultati. 
Queste quantità di interesse sono dette \textbf{variabili aleatorie}. Siccome il valore di questa variabile è dato dal risultato dell'esperimento, possiamo assegnare delle probabilità a queste. Queste variabili aleatorie hanno una \textit{funzione indicatrice} definita, ad esempio, così:
\begin{displaymath}
    I:=
    \begin{cases}
        1 \mbox{ se } X = 1 \mbox{ o } 2\\
        0 \mbox{ se } X = 0
    \end{cases}
\end{displaymath}
Variabili aleatorie con un numero finito o numerabile di valori possibili sono dette \textbf{discrete}. Esistono anche variabili aleatorie \textbf{continue}.
\subsubsection{Funzione di ripartizione}
La funzione di ripartizione F di una variabile aleatoria X, è definita, per ogni numero reale $x$, tramite
\begin{displaymath}
    F(x) := P(X\le x)
\end{displaymath}
Quindi $F(x)$ esprime la probabilità che la variabile aleatoria X assuma un valore \textit{minore o uguale} a $x$. Tutte le questioni di probabilità che si possano sollevare su una variabile aleatoria, ammettono una risposta in termini della sua funzione di ripartizione.
\subsection{Variabili aleatorie discrete e continue}
Se X è una variabile aleatoria discreta, la sua funzione di massa di probabilità, o funzione di massa, si definisce nel modo seguente:
\begin{displaymath}
    p(a) :=P(X=a)
\end{displaymath}
La funzione $p(a)$ è non nulla su un insieme al più numerabile di valori. Infatti, se $x_1,x_2,...,x_n$ sono i possibili valori di X, allora
\begin{gather*}
    p(x_i) > 0 \hspace{10px}i=1,2,...\\
    p(x) = 0 \hspace{10px}\mbox{tutti gli altri valori di x}
\end{gather*}
Siccome X deve assumere i suddetti valori, necessariamente deve essere vero che
\begin{displaymath}
    \sum_{i=1}^\infty p(x_i) = 1
\end{displaymath}
Una variabile aleatoria che possa assumere un'infinità non numerabile di valori, non potrà essere discreta. Si dirà \textbf{continua} se esiste una funzione non negativa $f$, definita su tutto $\mathbb{R}$, avente la proprietà che per ogni insieme B di numeri reali,
\begin{displaymath}
    P(X\in B) = \int_B f(x) dx
\end{displaymath}
Questa funzione è detta \textbf{funzione di densità di probabilità}. L'equazione dice che la probabilità che una variabile aleatoria continua X appartenga a un insieme B si può trovare integrando la sua densità su tale insieme. Pare ovvio che
\begin{displaymath}
    1=P(X\in \mathbb{R}) = \int_{-\infty}^{\infty} f(x)dx
\end{displaymath}
Tutte le probabilità che riguardano una variabile aleatoria continua possono essere espresse in funzione della sua densità di probabilità:
\begin{displaymath}
    P(a\le X \le b) = \int_a^b f(x)dx
\end{displaymath}
Se poniamo $b=a$, notiamo che la probabilità che una variabile aleatoria continua assuma un valore particolare $a$ è nulla:
\begin{displaymath}
    P(X=a) = \int_a^a f(x)dx = 0
\end{displaymath}
Leghiamo la funzione di ripartizione F alla densità $f$ così:
\begin{displaymath}
    F(a) := P(X \in (-\infty,a]) = \int_{-\infty}^a f(x)dx
\end{displaymath}
Derivando entrambi otteniamo la relazione fondamentale:
\begin{displaymath}
    \frac{d}{da}F(a) = f(a)
\end{displaymath}
La densità è quindi la derivata della funzione di ripartizione. 
Notiamo che quando conosciamo la funzione di massa di probabilità di una variabile aleatoria discreta, o la funzione di densità di probabilità di una continua, abbiamo abbastanza informazioni per poter calcolare le probabilità di ogni evento che dipenda dalla sola variabile aleatoria. 
\subsection{Coppie e vettori di variabili aleatorie}
Ci sono situazioni in cui abbiamo necessità di studiare le \textbf{relazioni} tra variabili aleatorie multiple. Per specificare la relazione tra due variabili aleatorie X e Y, il primo passo è estendere il concetto di funzione di ripartizione.
Siano quindi X e Y due variabili aleatorie che riguardano lo stesso esperimento casuale. Si dice \textit{funzione di ripartizione congiunta} di X e Y la funzione di due variabili seguente:
\begin{displaymath}
    F(x,y) := P(X\le x, Y \le y)
\end{displaymath}
dove la virgola denota l'intersezione tra gli eventi.
La conoscenza di questa funzione permette, almeno in teoria, di calcolare le probabilità di tutti gli eventi che dipendono, singolarmente o congiuntamente, da X e Y. 
\subsubsection{Distribuzione congiunta per variabili aleatorie discrete}
Se sappiamo che un vettore aleatorio è di tipo discreto, possiamo definire e utilizzare la funzione di massa di probabilità.
Se X e Y sono variabili aleatorie discrete che assumono i valori $x_1, x_2,...$ e $y_1,y_2,...$, la funzione
\begin{displaymath}
    p(x_i, y_j) := P(X=x_i, Y=y_j),\hspace{10px}i=1,2,...\hspace{5px}j=1,2,...
\end{displaymath}
è la loro funzione di massa di probabilità congiunta.
Le funzioni di massa individuali si possono ricavare da questa, notando che, siccome Y deve assumere uno dei valori $y_j$, l'evento $\{X=x_i\}$, può essere visto come l'unione al variare di $j$ degli eventi $\{X=x_i, Y=y_j\}$, che sono mutuamente esclusivi. Da qui:
\begin{displaymath}
    p_X(x_i) := P(X=x_i) = \sum_j p(x_i, y_j)
\end{displaymath}
Anche se le individuali possono essere ricavate dalla congiunta, la congiunta non può essere ricavata dalle condizionali. 
\subsubsection{Distribuzione congiunta per variabili aleatorie continue}
Due variabili aleatorie X e Y sono congiuntamente continue se esiste una funzione non negativa $f(x,y)$ tale che, per ogni sottoinsieme C del piano cartesiano,
\begin{displaymath}
    P((X,Y)\in C) = \int\int_{(x,y)\in C} f(x,y)dxdy
\end{displaymath}
questa è detta \textbf{densità congiunta} delle variabili aleatorie X e Y.
Otteniamo inoltre che
\begin{displaymath}
    P(X \in A, Y \in B) = \int_B\int_A f(x,y)dxdy
\end{displaymath} 
E, in conclusione,
\begin{displaymath}
    P(X \in A) = \int_A f_X(x) dx
\end{displaymath}
Per ricavare le individuali, otteniamo
\begin{gather*}
    f_X(x) =\int_{-\infty}^\infty f(x,y) dy\\
    f_Y(y) =\int_{-\infty}^\infty f(x,y) dx
\end{gather*}
\subsubsection{Variabili aleatorie indipendenti}
Due variabili aleatorie sono indipendenti se tutti gli eventi relativi alla prima sono indipendenti da tutti quelli relativi alla seconda. 
La definizione è che se, per ogni coppia di insiemi di numeri reali A e B è soddisfatta
\begin{displaymath}
    P(X \in A, Y \in B) = P(X\in A)P(Y \in B)
\end{displaymath}
le due V.A. sono indipendenti. 
Se le V.A. sono discrete, l'equazione equivale a dire che la funzione di massa congiunta è il prodotto delle marginali:
\begin{displaymath}
    p(x,y) = p_X(x) p_Y(y)
\end{displaymath}
Possiamo generalizzare le osservazioni suddette anche per vettori di variabili aleatorie. \textit{Lo faremo? Non credo proprio.}
\subsubsection{Distribuzioni condizionali}
Le relazioni esistenti tra due variabili aleatorie possono essere chiarite dallo studio della distribuzione condizionale di una delle due, dato il valore dell'altra. Si ricorda che che presi comunque due eventi E e F con P(F)>0, la probabilità di E condizionata a F è data da
\begin{displaymath}
    P(E|F):=\frac{P(E\cap F)}{P(F)}
\end{displaymath}
è naturale applicare questo schema anche alle variabili aleatorie discrete.
Siano X e Y due variabili aleatorie discrete con funzione di massa congiunta $p(\cdot , \cdot)$, diciamo funzione di massa di probabilità condizionata di X dato Y e si indica con $p_{X|Y}(\cdot | \cdot)$, la funzione di due variabili così definita:
\begin{gather*}
    p_{X|Y}(x|y) := P(X=x|Y=y)\\ 
    \frac{p(x,y)}{p_Y(y)},\hspace{10px}\forall x \forall y \mbox{ con }p_Y(y)>0
\end{gather*}
Se $y$ non è un valore possibile di Y, ovvero se $P(Y=y)=0$, la quantità $p_{X|Y}(x|y)$ non è definita. 
\subsection{Valore atteso}
Uno dei concetti più importanti di tutta la teoria della probabilità \textit{(ziocane)} è quello di valore atteso. Esso è definito come il numero
\begin{displaymath}
    E[X]:=\sum_i x_i P(X=x_i)
\end{displaymath}
In altri termini, si tratta della media pesata dei valori possibili di X, usando come pesi le probabilità che vengano assunti. È ovvio che nel caso di V.A. continue, il giochino non funziona. Definiamo quindi il valore atteso di una V.A. continua con funzione di densità $f$, come
\begin{displaymath}
    E[X]:=\int_{-\infty}^\infty xf(x)dx
\end{displaymath}
\subsection{Proprietà del valore atteso}
Consideriamo una V.A. di cui conosciamo la distribuzione. \textit{What if, }se anziché calcolare il valore atteso di X, volessimo calcolare quello di una funzione $g(X)$? Notiamo che $g(X)$ è comunque una variabile aleatoria. Ricaviamo quindi la sua distribuzione, e ne calcoliamo il valore atteso. Ponendo le cose in maniera rigorosa:
\begin{gather*}
E[g(X)] = \sum_x g(x)p(x)\mbox{ per V.A. discrete}\\ 
E[g(X)] =\int_{-\infty}^\infty g(x)f(x)dx\mbox{ per V.A. continue}
\end{gather*}
Per ogni coppia di costanti reali $a$ e $b$, abbiamo anche che
\begin{displaymath}
    E[aX+b] = aE[X]+b\hspace{10px}\mbox{e quindi}\hspace{10px}E[aX]=aE[X]
\end{displaymath}
\subsubsection{Valore atteso della somma di variabili aleatorie}
Con \textit{complessi calcoli tendenzialmente inutili}, otteniamo che 
\begin{displaymath}
    E[X+Y] = E[X] + E[Y]
\end{displaymath}
Tale risultato vale sia nel caso discreto, che in quello continuo.
\subsection{Varianza}
A volte, conoscere la media di una distribuzione non basta. \textit{Se inserissimo l'autore di questi riassunti con la testa in un freezer e i piedi in un forno, la tempratura media sarebbe abbastanza ok, l'autore no.} Per questo, è utile conoscere quanto i valori si allontanano dalla media. Questo è proprio il compito della \textbf{varianza}. Sia X una variabile aleatoria con media $\mu$, la varianza di x, che denotiamo con $Var(X)$ è la quantità
\begin{displaymath}
    Var(X):=E[(X-\mu)^2]
\end{displaymath}
o, in alternativa (\textit{questa è molto più comoda})
\begin{displaymath}
    Var(X) = E[X^2]-E[X]^2
\end{displaymath}
\subsection{La covarianza e la varianza della somma di V.A.}
Come sappiamo, la media della somma di V.A. coincide con la somma delle loro medie. Per la varianza, in generale, questo non è vero. \textbf{In un caso sì: quando le V.A. sono indipendenti}. Prima di tutto, però, definiamo il concetto di Covarianza: date due V.A. X e Y di media $\mu_X$ e $\mu_Y$, essa vale
\begin{displaymath}
    Cov(X,Y) := E[(X-\mu_X)(Y-\mu_Y)]
\end{displaymath}
o, in alternativa
\begin{displaymath}
    Cov(X,Y) := E[XY] - E[X]E[Y]
\end{displaymath}
Derivano anche alcune semplici proprietà
\begin{gather*}
    Cov(X,Y)=Cov(Y,X)\\ 
    Cov(X,X)=Var(X)\\ 
    Cov(aX,Y)=aCov(X,Y)=Cov(X,aY)
\end{gather*}
E se avessimo 3 V.A.?
\begin{displaymath}
    Cov(X+Y,Z) = Cov(X,Z)+Cov(Y,Z)
\end{displaymath}
Inoltre, generalizzando i concetti, se avessimo n V.A. $X_1,...,X_n$ e n $Y_1,...,Y_n$
\begin{displaymath}
    Cov\left(\sum_{i=1}^nX_i,\sum_{j=1}^mY_j\right) = \sum_{i=1}^n\sum_{j=1}^m Cov(X_i,Y_j)
\end{displaymath}
\subsubsection{Variabili aleatorie indipendenti}
Se abbiamo due V.A. X e Y indipendenti, sappiamo che
\begin{displaymath}
    E[XY] = E[X]E[Y]
\end{displaymath}
Questo implica inoltre che
\begin{displaymath}
    Cov(X,Y) = 0
\end{displaymath}
e quindi, se abbiamo n V.A., la varianza della somma è la somma delle varianze.
\begin{displaymath}
    Var(\sum_{i=1}^n X_i) = \sum_{i=1}^n Var(X_i)
\end{displaymath}
\section{La funzione generatrice dei momenti}
La \textit{funzione generatrice dei momenti}, o più semplicemente, la funzione generatrice $\phi$ di una V.A. X, è definita, per tutti i $t$ reali per i quali il valore atteso di $e^{tX}$ ha senso, dall'espressione
\begin{displaymath}
    \phi(t):=E[e^{tX}]=
    \begin{cases}
        \sum_x e^{tx}p(x)\hspace{10px}\mbox{se X è discreta}\\ 
        \int_{-\infty}^\infty e^{tx} f(x)dx\hspace{10px}\mbox{se X è continua}
    \end{cases}
\end{displaymath} 
Il nome deriva dal fatto che tutti i momenti di cui è dotata X possono essere ottenuti derivando più volte nell'origine la funzione $\phi(t)$. Ad esempio,
\begin{displaymath}
    \phi'(t)=\frac{d}{dt}E[e^{tX}]=E[Xe^{tX}]
\end{displaymath}
Quindi, $\phi'(0) = E[X]$, e, più in generale, $\phi^n(0) = E[X^n]$.
Se X e Y sono variabili indipendenti con funzioni generatrici $\phi_X$ e $\phi_Y$, e se $\phi_{X+Y}$ è la funzione generatrice dei momenti di $X+Y$, allora
\begin{displaymath}
    \phi_{X+Y}(t) = \phi_X(t)\phi_Y(t)
\end{displaymath}
Un'osservazione interessante sulla generatrice dei momenti, è che essa \textit{determina la distribuzione}, ossia se due V.A. hanno identica generatrice, hanno identica legge(quindi funzione di ripartizione e funzione di massa).
\section{La legge debole dei grandi numeri}
Per introdurre la suddetta, prima enunciamo la \textbf{disuguaglianza di Markov}: se X è una variabile aleatoria che non è mai negativa. allora per ogni $a>0$
\begin{displaymath}
    P(X\ge a) \le \frac{E[X]}{a}
\end{displaymath}
Come corollario, ricaviamo la disuguaglianza di Chebyshev: data una V.A. X con media $\mu$ e varianza $\sigma^2$, allora per ogni $r>0$
\begin{displaymath}
    P(|X-\mu|\ge r) \le \frac{\sigma^2}{r^2}
\end{displaymath}
Otteniamo infine la \textbf{legge debole dei grandi numeri}. \textit{No, non quella con cui giustificate il vostro provarci con ogni essere vivente femminile.} Sia $X_1,X_2,...$ una successione di variabili aleatorie i.i.d\textit{(indipendenti, identicamente distribuite)}, tutte con media $\mu$. Allora, per ogni $\epsilon >0$:
\begin{displaymath}
    P\left(\left|\frac{X_1+...+X_n}{n} - \mu\right|>\epsilon \right)\rightarrow0\hspace{10px}\mbox{quando}n\rightarrow\infty
\end{displaymath}
\textit{Ora, vi chiederete: cosa me ne faccio? Posso spiegarla alle tipe in discoteca?\textbf{No.}} Una applicazione interessante è la seguente: supponiamo di ripetere in successione molte copie indipendenti di un esperimento, in ciascuna delle quali può verificarsi un certo evento E:
\begin{displaymath}
    X_i:=
    \begin{cases}
        1\mbox{ se E si realizza nell'esperimento i-esimo}\\ 
        0\mbox{ se E non si realizza}
    \end{cases}
\end{displaymath}
la sommatoria $X_1+...+X_n$ rappresenta il numero di prove - tra le prime n - in cui si è verificato l'evento E. Poiché 
\begin{displaymath}
    E[X_i]=P(X_i=1)=P(E)
\end{displaymath}
si deduce che la frazione delle \textit{n} prove nelle quali si realizza E, tende alla probabilità $P(E)$.
\end{document}
